{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1414,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01415929203539823,
      "grad_norm": 0.6002803444862366,
      "learning_rate": 4.9787535410764873e-05,
      "loss": 1.8994,
      "step": 10
    },
    {
      "epoch": 0.02831858407079646,
      "grad_norm": 0.8372678160667419,
      "learning_rate": 4.9551463644948064e-05,
      "loss": 1.7699,
      "step": 20
    },
    {
      "epoch": 0.04247787610619469,
      "grad_norm": 0.7904869914054871,
      "learning_rate": 4.9315391879131255e-05,
      "loss": 1.5223,
      "step": 30
    },
    {
      "epoch": 0.05663716814159292,
      "grad_norm": 0.44287124276161194,
      "learning_rate": 4.9079320113314446e-05,
      "loss": 1.3668,
      "step": 40
    },
    {
      "epoch": 0.07079646017699115,
      "grad_norm": 0.5159350633621216,
      "learning_rate": 4.884324834749764e-05,
      "loss": 1.3055,
      "step": 50
    },
    {
      "epoch": 0.08495575221238938,
      "grad_norm": 0.412307471036911,
      "learning_rate": 4.8607176581680834e-05,
      "loss": 1.2721,
      "step": 60
    },
    {
      "epoch": 0.09911504424778761,
      "grad_norm": 0.4920317232608795,
      "learning_rate": 4.8371104815864025e-05,
      "loss": 1.2537,
      "step": 70
    },
    {
      "epoch": 0.11327433628318584,
      "grad_norm": 0.4459075331687927,
      "learning_rate": 4.8135033050047215e-05,
      "loss": 1.2447,
      "step": 80
    },
    {
      "epoch": 0.12743362831858407,
      "grad_norm": 0.45253387093544006,
      "learning_rate": 4.7898961284230406e-05,
      "loss": 1.2661,
      "step": 90
    },
    {
      "epoch": 0.1415929203539823,
      "grad_norm": 0.49597468972206116,
      "learning_rate": 4.7662889518413604e-05,
      "loss": 1.3221,
      "step": 100
    },
    {
      "epoch": 0.15575221238938053,
      "grad_norm": 0.43525004386901855,
      "learning_rate": 4.7426817752596795e-05,
      "loss": 1.2543,
      "step": 110
    },
    {
      "epoch": 0.16991150442477876,
      "grad_norm": 0.43994957208633423,
      "learning_rate": 4.7190745986779985e-05,
      "loss": 1.2519,
      "step": 120
    },
    {
      "epoch": 0.184070796460177,
      "grad_norm": 0.4637891948223114,
      "learning_rate": 4.6954674220963176e-05,
      "loss": 1.2226,
      "step": 130
    },
    {
      "epoch": 0.19823008849557522,
      "grad_norm": 0.42566266655921936,
      "learning_rate": 4.671860245514637e-05,
      "loss": 1.261,
      "step": 140
    },
    {
      "epoch": 0.21238938053097345,
      "grad_norm": 0.5060666799545288,
      "learning_rate": 4.648253068932956e-05,
      "loss": 1.2235,
      "step": 150
    },
    {
      "epoch": 0.22654867256637168,
      "grad_norm": 0.4051678478717804,
      "learning_rate": 4.624645892351275e-05,
      "loss": 1.1965,
      "step": 160
    },
    {
      "epoch": 0.2407079646017699,
      "grad_norm": 0.5324794054031372,
      "learning_rate": 4.6010387157695946e-05,
      "loss": 1.2226,
      "step": 170
    },
    {
      "epoch": 0.25486725663716814,
      "grad_norm": 0.5153335928916931,
      "learning_rate": 4.5774315391879137e-05,
      "loss": 1.2461,
      "step": 180
    },
    {
      "epoch": 0.26902654867256637,
      "grad_norm": 0.4866899847984314,
      "learning_rate": 4.553824362606233e-05,
      "loss": 1.2184,
      "step": 190
    },
    {
      "epoch": 0.2831858407079646,
      "grad_norm": 0.4806630313396454,
      "learning_rate": 4.530217186024552e-05,
      "loss": 1.1984,
      "step": 200
    },
    {
      "epoch": 0.2973451327433628,
      "grad_norm": 0.4258778691291809,
      "learning_rate": 4.506610009442871e-05,
      "loss": 1.25,
      "step": 210
    },
    {
      "epoch": 0.31150442477876106,
      "grad_norm": 0.4819760322570801,
      "learning_rate": 4.48300283286119e-05,
      "loss": 1.2128,
      "step": 220
    },
    {
      "epoch": 0.3256637168141593,
      "grad_norm": 0.46180063486099243,
      "learning_rate": 4.459395656279509e-05,
      "loss": 1.2339,
      "step": 230
    },
    {
      "epoch": 0.3398230088495575,
      "grad_norm": 0.48273250460624695,
      "learning_rate": 4.435788479697828e-05,
      "loss": 1.2355,
      "step": 240
    },
    {
      "epoch": 0.35398230088495575,
      "grad_norm": 0.5231866240501404,
      "learning_rate": 4.412181303116147e-05,
      "loss": 1.2198,
      "step": 250
    },
    {
      "epoch": 0.368141592920354,
      "grad_norm": 0.5037385821342468,
      "learning_rate": 4.388574126534466e-05,
      "loss": 1.222,
      "step": 260
    },
    {
      "epoch": 0.3823008849557522,
      "grad_norm": 0.4840356409549713,
      "learning_rate": 4.3649669499527853e-05,
      "loss": 1.1879,
      "step": 270
    },
    {
      "epoch": 0.39646017699115044,
      "grad_norm": 0.5632855296134949,
      "learning_rate": 4.341359773371105e-05,
      "loss": 1.1889,
      "step": 280
    },
    {
      "epoch": 0.41061946902654867,
      "grad_norm": 0.543155312538147,
      "learning_rate": 4.317752596789424e-05,
      "loss": 1.229,
      "step": 290
    },
    {
      "epoch": 0.4247787610619469,
      "grad_norm": 0.5524522066116333,
      "learning_rate": 4.294145420207743e-05,
      "loss": 1.155,
      "step": 300
    },
    {
      "epoch": 0.4389380530973451,
      "grad_norm": 0.4879978895187378,
      "learning_rate": 4.270538243626063e-05,
      "loss": 1.2414,
      "step": 310
    },
    {
      "epoch": 0.45309734513274336,
      "grad_norm": 0.5435587763786316,
      "learning_rate": 4.246931067044382e-05,
      "loss": 1.2602,
      "step": 320
    },
    {
      "epoch": 0.4672566371681416,
      "grad_norm": 0.6195966005325317,
      "learning_rate": 4.223323890462701e-05,
      "loss": 1.2631,
      "step": 330
    },
    {
      "epoch": 0.4814159292035398,
      "grad_norm": 0.5325483679771423,
      "learning_rate": 4.19971671388102e-05,
      "loss": 1.2138,
      "step": 340
    },
    {
      "epoch": 0.49557522123893805,
      "grad_norm": 0.5303341746330261,
      "learning_rate": 4.176109537299339e-05,
      "loss": 1.2729,
      "step": 350
    },
    {
      "epoch": 0.5097345132743363,
      "grad_norm": 0.5033726096153259,
      "learning_rate": 4.1525023607176584e-05,
      "loss": 1.2563,
      "step": 360
    },
    {
      "epoch": 0.5238938053097345,
      "grad_norm": 0.5167916417121887,
      "learning_rate": 4.1288951841359775e-05,
      "loss": 1.2134,
      "step": 370
    },
    {
      "epoch": 0.5380530973451327,
      "grad_norm": 0.6657823324203491,
      "learning_rate": 4.1052880075542965e-05,
      "loss": 1.1998,
      "step": 380
    },
    {
      "epoch": 0.552212389380531,
      "grad_norm": 0.5898147225379944,
      "learning_rate": 4.0816808309726156e-05,
      "loss": 1.216,
      "step": 390
    },
    {
      "epoch": 0.5663716814159292,
      "grad_norm": 0.5561444163322449,
      "learning_rate": 4.058073654390935e-05,
      "loss": 1.1963,
      "step": 400
    },
    {
      "epoch": 0.5805309734513274,
      "grad_norm": 0.5273023247718811,
      "learning_rate": 4.0344664778092544e-05,
      "loss": 1.1501,
      "step": 410
    },
    {
      "epoch": 0.5946902654867257,
      "grad_norm": 0.5796951651573181,
      "learning_rate": 4.0108593012275735e-05,
      "loss": 1.1702,
      "step": 420
    },
    {
      "epoch": 0.6088495575221239,
      "grad_norm": 0.5083635449409485,
      "learning_rate": 3.9872521246458926e-05,
      "loss": 1.1902,
      "step": 430
    },
    {
      "epoch": 0.6230088495575221,
      "grad_norm": 0.5843495726585388,
      "learning_rate": 3.963644948064212e-05,
      "loss": 1.2182,
      "step": 440
    },
    {
      "epoch": 0.6371681415929203,
      "grad_norm": 0.6568189859390259,
      "learning_rate": 3.940037771482531e-05,
      "loss": 1.2314,
      "step": 450
    },
    {
      "epoch": 0.6513274336283186,
      "grad_norm": 0.5582744479179382,
      "learning_rate": 3.91643059490085e-05,
      "loss": 1.2208,
      "step": 460
    },
    {
      "epoch": 0.6654867256637168,
      "grad_norm": 0.613336443901062,
      "learning_rate": 3.892823418319169e-05,
      "loss": 1.2834,
      "step": 470
    },
    {
      "epoch": 0.679646017699115,
      "grad_norm": 0.5596470236778259,
      "learning_rate": 3.8692162417374886e-05,
      "loss": 1.2006,
      "step": 480
    },
    {
      "epoch": 0.6938053097345133,
      "grad_norm": 0.5938729643821716,
      "learning_rate": 3.845609065155808e-05,
      "loss": 1.1593,
      "step": 490
    },
    {
      "epoch": 0.7079646017699115,
      "grad_norm": 0.6184684634208679,
      "learning_rate": 3.822001888574127e-05,
      "loss": 1.2383,
      "step": 500
    },
    {
      "epoch": 0.7221238938053097,
      "grad_norm": 0.5908363461494446,
      "learning_rate": 3.798394711992446e-05,
      "loss": 1.2124,
      "step": 510
    },
    {
      "epoch": 0.736283185840708,
      "grad_norm": 0.592547595500946,
      "learning_rate": 3.774787535410765e-05,
      "loss": 1.1858,
      "step": 520
    },
    {
      "epoch": 0.7504424778761062,
      "grad_norm": 0.5176660418510437,
      "learning_rate": 3.751180358829085e-05,
      "loss": 1.2101,
      "step": 530
    },
    {
      "epoch": 0.7646017699115044,
      "grad_norm": 0.5728599429130554,
      "learning_rate": 3.727573182247404e-05,
      "loss": 1.203,
      "step": 540
    },
    {
      "epoch": 0.7787610619469026,
      "grad_norm": 0.559916079044342,
      "learning_rate": 3.703966005665723e-05,
      "loss": 1.216,
      "step": 550
    },
    {
      "epoch": 0.7929203539823009,
      "grad_norm": 0.6526515483856201,
      "learning_rate": 3.680358829084042e-05,
      "loss": 1.1734,
      "step": 560
    },
    {
      "epoch": 0.8070796460176991,
      "grad_norm": 0.6321780681610107,
      "learning_rate": 3.656751652502361e-05,
      "loss": 1.2365,
      "step": 570
    },
    {
      "epoch": 0.8212389380530973,
      "grad_norm": 0.5667020082473755,
      "learning_rate": 3.63314447592068e-05,
      "loss": 1.2256,
      "step": 580
    },
    {
      "epoch": 0.8353982300884956,
      "grad_norm": 0.6406606435775757,
      "learning_rate": 3.609537299338999e-05,
      "loss": 1.22,
      "step": 590
    },
    {
      "epoch": 0.8495575221238938,
      "grad_norm": 0.617275595664978,
      "learning_rate": 3.585930122757318e-05,
      "loss": 1.2128,
      "step": 600
    },
    {
      "epoch": 0.863716814159292,
      "grad_norm": 0.5522498488426208,
      "learning_rate": 3.562322946175637e-05,
      "loss": 1.2045,
      "step": 610
    },
    {
      "epoch": 0.8778761061946903,
      "grad_norm": 0.599518895149231,
      "learning_rate": 3.5387157695939564e-05,
      "loss": 1.1979,
      "step": 620
    },
    {
      "epoch": 0.8920353982300885,
      "grad_norm": 0.6753330230712891,
      "learning_rate": 3.5151085930122755e-05,
      "loss": 1.2199,
      "step": 630
    },
    {
      "epoch": 0.9061946902654867,
      "grad_norm": 0.5639086961746216,
      "learning_rate": 3.4915014164305945e-05,
      "loss": 1.1818,
      "step": 640
    },
    {
      "epoch": 0.9203539823008849,
      "grad_norm": 0.5983591079711914,
      "learning_rate": 3.467894239848914e-05,
      "loss": 1.2029,
      "step": 650
    },
    {
      "epoch": 0.9345132743362832,
      "grad_norm": 0.6112005114555359,
      "learning_rate": 3.4442870632672334e-05,
      "loss": 1.1931,
      "step": 660
    },
    {
      "epoch": 0.9486725663716814,
      "grad_norm": 0.606945276260376,
      "learning_rate": 3.4206798866855524e-05,
      "loss": 1.1001,
      "step": 670
    },
    {
      "epoch": 0.9628318584070796,
      "grad_norm": 0.5534370541572571,
      "learning_rate": 3.3970727101038715e-05,
      "loss": 1.1406,
      "step": 680
    },
    {
      "epoch": 0.9769911504424779,
      "grad_norm": 0.6160436272621155,
      "learning_rate": 3.373465533522191e-05,
      "loss": 1.196,
      "step": 690
    },
    {
      "epoch": 0.9911504424778761,
      "grad_norm": 0.6889746785163879,
      "learning_rate": 3.3498583569405103e-05,
      "loss": 1.1781,
      "step": 700
    },
    {
      "epoch": 1.0042477876106195,
      "grad_norm": 0.5896530151367188,
      "learning_rate": 3.3262511803588294e-05,
      "loss": 1.2,
      "step": 710
    },
    {
      "epoch": 1.0184070796460176,
      "grad_norm": 0.6204215884208679,
      "learning_rate": 3.3026440037771485e-05,
      "loss": 1.2141,
      "step": 720
    },
    {
      "epoch": 1.032566371681416,
      "grad_norm": 0.5923598408699036,
      "learning_rate": 3.2790368271954676e-05,
      "loss": 1.1576,
      "step": 730
    },
    {
      "epoch": 1.046725663716814,
      "grad_norm": 0.6358673572540283,
      "learning_rate": 3.2554296506137866e-05,
      "loss": 1.1916,
      "step": 740
    },
    {
      "epoch": 1.0608849557522124,
      "grad_norm": 0.5691366195678711,
      "learning_rate": 3.231822474032106e-05,
      "loss": 1.2037,
      "step": 750
    },
    {
      "epoch": 1.0750442477876105,
      "grad_norm": 0.6228576302528381,
      "learning_rate": 3.208215297450425e-05,
      "loss": 1.1833,
      "step": 760
    },
    {
      "epoch": 1.0892035398230089,
      "grad_norm": 0.6421353816986084,
      "learning_rate": 3.1846081208687446e-05,
      "loss": 1.2513,
      "step": 770
    },
    {
      "epoch": 1.103362831858407,
      "grad_norm": 0.6479482650756836,
      "learning_rate": 3.1610009442870636e-05,
      "loss": 1.1716,
      "step": 780
    },
    {
      "epoch": 1.1175221238938053,
      "grad_norm": 0.6372014284133911,
      "learning_rate": 3.137393767705383e-05,
      "loss": 1.1644,
      "step": 790
    },
    {
      "epoch": 1.1316814159292035,
      "grad_norm": 0.6585227847099304,
      "learning_rate": 3.113786591123702e-05,
      "loss": 1.1203,
      "step": 800
    },
    {
      "epoch": 1.1458407079646018,
      "grad_norm": 0.638034999370575,
      "learning_rate": 3.090179414542021e-05,
      "loss": 1.153,
      "step": 810
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.6256441473960876,
      "learning_rate": 3.06657223796034e-05,
      "loss": 1.1812,
      "step": 820
    },
    {
      "epoch": 1.1741592920353983,
      "grad_norm": 0.62055504322052,
      "learning_rate": 3.0429650613786593e-05,
      "loss": 1.2084,
      "step": 830
    },
    {
      "epoch": 1.1883185840707964,
      "grad_norm": 0.6429876089096069,
      "learning_rate": 3.0193578847969784e-05,
      "loss": 1.149,
      "step": 840
    },
    {
      "epoch": 1.2024778761061947,
      "grad_norm": 0.7297533750534058,
      "learning_rate": 2.9957507082152975e-05,
      "loss": 1.1601,
      "step": 850
    },
    {
      "epoch": 1.2166371681415928,
      "grad_norm": 0.7487120628356934,
      "learning_rate": 2.9721435316336166e-05,
      "loss": 1.1574,
      "step": 860
    },
    {
      "epoch": 1.2307964601769912,
      "grad_norm": 0.6871541738510132,
      "learning_rate": 2.9485363550519356e-05,
      "loss": 1.1862,
      "step": 870
    },
    {
      "epoch": 1.2449557522123893,
      "grad_norm": 0.759599506855011,
      "learning_rate": 2.9249291784702547e-05,
      "loss": 1.1988,
      "step": 880
    },
    {
      "epoch": 1.2591150442477876,
      "grad_norm": 0.7399051785469055,
      "learning_rate": 2.9013220018885745e-05,
      "loss": 1.1184,
      "step": 890
    },
    {
      "epoch": 1.2732743362831858,
      "grad_norm": 0.5855377912521362,
      "learning_rate": 2.8777148253068936e-05,
      "loss": 1.1759,
      "step": 900
    },
    {
      "epoch": 1.287433628318584,
      "grad_norm": 0.6945554614067078,
      "learning_rate": 2.8541076487252126e-05,
      "loss": 1.1875,
      "step": 910
    },
    {
      "epoch": 1.3015929203539822,
      "grad_norm": 0.724428117275238,
      "learning_rate": 2.8305004721435317e-05,
      "loss": 1.1821,
      "step": 920
    },
    {
      "epoch": 1.3157522123893806,
      "grad_norm": 0.6438061594963074,
      "learning_rate": 2.806893295561851e-05,
      "loss": 1.1853,
      "step": 930
    },
    {
      "epoch": 1.3299115044247787,
      "grad_norm": 0.6938403844833374,
      "learning_rate": 2.7832861189801702e-05,
      "loss": 1.1743,
      "step": 940
    },
    {
      "epoch": 1.344070796460177,
      "grad_norm": 0.7065117955207825,
      "learning_rate": 2.7596789423984893e-05,
      "loss": 1.2229,
      "step": 950
    },
    {
      "epoch": 1.3582300884955751,
      "grad_norm": 0.7359750866889954,
      "learning_rate": 2.7360717658168083e-05,
      "loss": 1.1644,
      "step": 960
    },
    {
      "epoch": 1.3723893805309735,
      "grad_norm": 0.7601286768913269,
      "learning_rate": 2.7124645892351274e-05,
      "loss": 1.1916,
      "step": 970
    },
    {
      "epoch": 1.3865486725663716,
      "grad_norm": 0.7578590512275696,
      "learning_rate": 2.6888574126534465e-05,
      "loss": 1.1877,
      "step": 980
    },
    {
      "epoch": 1.40070796460177,
      "grad_norm": 0.7167697548866272,
      "learning_rate": 2.6652502360717656e-05,
      "loss": 1.2353,
      "step": 990
    },
    {
      "epoch": 1.414867256637168,
      "grad_norm": 0.7228803038597107,
      "learning_rate": 2.641643059490085e-05,
      "loss": 1.161,
      "step": 1000
    },
    {
      "epoch": 1.4290265486725664,
      "grad_norm": 0.70632404088974,
      "learning_rate": 2.6180358829084044e-05,
      "loss": 1.1596,
      "step": 1010
    },
    {
      "epoch": 1.4431858407079647,
      "grad_norm": 0.7670905590057373,
      "learning_rate": 2.5944287063267235e-05,
      "loss": 1.1652,
      "step": 1020
    },
    {
      "epoch": 1.4573451327433629,
      "grad_norm": 0.741479218006134,
      "learning_rate": 2.570821529745043e-05,
      "loss": 1.1754,
      "step": 1030
    },
    {
      "epoch": 1.471504424778761,
      "grad_norm": 0.788620114326477,
      "learning_rate": 2.547214353163362e-05,
      "loss": 1.2004,
      "step": 1040
    },
    {
      "epoch": 1.4856637168141593,
      "grad_norm": 0.7231153249740601,
      "learning_rate": 2.523607176581681e-05,
      "loss": 1.1779,
      "step": 1050
    },
    {
      "epoch": 1.4998230088495577,
      "grad_norm": 0.7907172441482544,
      "learning_rate": 2.5e-05,
      "loss": 1.2302,
      "step": 1060
    },
    {
      "epoch": 1.5139823008849558,
      "grad_norm": 0.7283062934875488,
      "learning_rate": 2.4763928234183192e-05,
      "loss": 1.2052,
      "step": 1070
    },
    {
      "epoch": 1.528141592920354,
      "grad_norm": 0.7417558431625366,
      "learning_rate": 2.4527856468366383e-05,
      "loss": 1.1687,
      "step": 1080
    },
    {
      "epoch": 1.5423008849557522,
      "grad_norm": 0.8750931620597839,
      "learning_rate": 2.4291784702549577e-05,
      "loss": 1.1998,
      "step": 1090
    },
    {
      "epoch": 1.5564601769911506,
      "grad_norm": 0.8064334988594055,
      "learning_rate": 2.4055712936732768e-05,
      "loss": 1.1621,
      "step": 1100
    },
    {
      "epoch": 1.5706194690265487,
      "grad_norm": 0.6817964911460876,
      "learning_rate": 2.381964117091596e-05,
      "loss": 1.2135,
      "step": 1110
    },
    {
      "epoch": 1.5847787610619468,
      "grad_norm": 0.7521230578422546,
      "learning_rate": 2.3583569405099153e-05,
      "loss": 1.1545,
      "step": 1120
    },
    {
      "epoch": 1.5989380530973452,
      "grad_norm": 0.7128072381019592,
      "learning_rate": 2.3347497639282343e-05,
      "loss": 1.1209,
      "step": 1130
    },
    {
      "epoch": 1.6130973451327435,
      "grad_norm": 0.7661571502685547,
      "learning_rate": 2.3111425873465534e-05,
      "loss": 1.1892,
      "step": 1140
    },
    {
      "epoch": 1.6272566371681416,
      "grad_norm": 0.7228063941001892,
      "learning_rate": 2.2875354107648728e-05,
      "loss": 1.236,
      "step": 1150
    },
    {
      "epoch": 1.6414159292035397,
      "grad_norm": 0.6955133676528931,
      "learning_rate": 2.263928234183192e-05,
      "loss": 1.1121,
      "step": 1160
    },
    {
      "epoch": 1.655575221238938,
      "grad_norm": 0.7562775015830994,
      "learning_rate": 2.240321057601511e-05,
      "loss": 1.1805,
      "step": 1170
    },
    {
      "epoch": 1.6697345132743364,
      "grad_norm": 0.8234621286392212,
      "learning_rate": 2.21671388101983e-05,
      "loss": 1.2201,
      "step": 1180
    },
    {
      "epoch": 1.6838938053097345,
      "grad_norm": 0.7718812823295593,
      "learning_rate": 2.193106704438149e-05,
      "loss": 1.1842,
      "step": 1190
    },
    {
      "epoch": 1.6980530973451327,
      "grad_norm": 0.7624548077583313,
      "learning_rate": 2.1694995278564685e-05,
      "loss": 1.18,
      "step": 1200
    },
    {
      "epoch": 1.712212389380531,
      "grad_norm": 0.7240909934043884,
      "learning_rate": 2.1458923512747876e-05,
      "loss": 1.1429,
      "step": 1210
    },
    {
      "epoch": 1.7263716814159293,
      "grad_norm": 0.8022968173027039,
      "learning_rate": 2.122285174693107e-05,
      "loss": 1.166,
      "step": 1220
    },
    {
      "epoch": 1.7405309734513275,
      "grad_norm": 0.7645841836929321,
      "learning_rate": 2.098677998111426e-05,
      "loss": 1.1911,
      "step": 1230
    },
    {
      "epoch": 1.7546902654867256,
      "grad_norm": 0.7778493165969849,
      "learning_rate": 2.0750708215297452e-05,
      "loss": 1.1762,
      "step": 1240
    },
    {
      "epoch": 1.768849557522124,
      "grad_norm": 0.7487038373947144,
      "learning_rate": 2.0514636449480643e-05,
      "loss": 1.1312,
      "step": 1250
    },
    {
      "epoch": 1.7830088495575223,
      "grad_norm": 0.8242776989936829,
      "learning_rate": 2.0278564683663833e-05,
      "loss": 1.1579,
      "step": 1260
    },
    {
      "epoch": 1.7971681415929204,
      "grad_norm": 0.7193605303764343,
      "learning_rate": 2.0042492917847027e-05,
      "loss": 1.2068,
      "step": 1270
    },
    {
      "epoch": 1.8113274336283185,
      "grad_norm": 0.6995705962181091,
      "learning_rate": 1.9806421152030218e-05,
      "loss": 1.1314,
      "step": 1280
    },
    {
      "epoch": 1.8254867256637168,
      "grad_norm": 0.7710381150245667,
      "learning_rate": 1.957034938621341e-05,
      "loss": 1.2621,
      "step": 1290
    },
    {
      "epoch": 1.8396460176991152,
      "grad_norm": 0.770742654800415,
      "learning_rate": 1.93342776203966e-05,
      "loss": 1.1809,
      "step": 1300
    },
    {
      "epoch": 1.8538053097345133,
      "grad_norm": 0.7882704734802246,
      "learning_rate": 1.9098205854579794e-05,
      "loss": 1.1766,
      "step": 1310
    },
    {
      "epoch": 1.8679646017699114,
      "grad_norm": 0.7796449065208435,
      "learning_rate": 1.8862134088762985e-05,
      "loss": 1.199,
      "step": 1320
    },
    {
      "epoch": 1.8821238938053098,
      "grad_norm": 0.70654296875,
      "learning_rate": 1.862606232294618e-05,
      "loss": 1.1866,
      "step": 1330
    },
    {
      "epoch": 1.896283185840708,
      "grad_norm": 0.8455727696418762,
      "learning_rate": 1.838999055712937e-05,
      "loss": 1.1586,
      "step": 1340
    },
    {
      "epoch": 1.9104424778761062,
      "grad_norm": 0.7543686032295227,
      "learning_rate": 1.815391879131256e-05,
      "loss": 1.145,
      "step": 1350
    },
    {
      "epoch": 1.9246017699115043,
      "grad_norm": 0.7257655262947083,
      "learning_rate": 1.791784702549575e-05,
      "loss": 1.1363,
      "step": 1360
    },
    {
      "epoch": 1.9387610619469027,
      "grad_norm": 0.821959376335144,
      "learning_rate": 1.7681775259678942e-05,
      "loss": 1.1922,
      "step": 1370
    },
    {
      "epoch": 1.952920353982301,
      "grad_norm": 0.764783501625061,
      "learning_rate": 1.7445703493862133e-05,
      "loss": 1.1905,
      "step": 1380
    },
    {
      "epoch": 1.9670796460176991,
      "grad_norm": 0.7339093089103699,
      "learning_rate": 1.7209631728045327e-05,
      "loss": 1.1605,
      "step": 1390
    },
    {
      "epoch": 1.9812389380530973,
      "grad_norm": 0.7158324122428894,
      "learning_rate": 1.6973559962228517e-05,
      "loss": 1.162,
      "step": 1400
    },
    {
      "epoch": 1.9953982300884956,
      "grad_norm": 0.7801815867424011,
      "learning_rate": 1.673748819641171e-05,
      "loss": 1.1377,
      "step": 1410
    }
  ],
  "logging_steps": 10,
  "max_steps": 2118,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.6574005623403315e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
