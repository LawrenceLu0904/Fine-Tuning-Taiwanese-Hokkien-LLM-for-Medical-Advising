{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an MLOps Pipeline\n",
    "\n",
    "In [Cloud Computing on Chameleon](https://teaching-on-testbeds.github.io/cloud-chi/), following the premise:\n",
    "\n",
    "> You are working at a machine learning engineer at a small startup company called GourmetGram. They are developing an online photo sharing community focused on food. You are testing a new model you have developed that automatically classifies photos of food into one of a set of categories: Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit. You have built a simple web application with which to test your model and get feedback from others.\n",
    "\n",
    "we deployed a basic machine learning service to an OpenStack cloud. However, that deployment involved a lot of manual steps (“ClickOps”), and any updates to it would similarly involve lots of manual effort, be difficult to track, etc.\n",
    "\n",
    "In this tutorial, we will learn how to automate both the initial deployment, and updates during the lifecycle of the application. We will:\n",
    "\n",
    "-   practice deploying systems following infrastructure-as-code and configuration-as-code principles using automated deployment tools\n",
    "-   and create an automated pipeline to manage a machine learning model through its lifecycle\n",
    "\n",
    "Our experiment will use the following automated deployment and lifecycle management tools:\n",
    "\n",
    "-   Terraform: A declarative Infrastructure as Code (IaC) tool used to provision and manage cloud infrastructure (servers, networks, etc.) by defining the desired end state in configuration files. Here, we use it to provision our infrastructure.\n",
    "-   Ansible: An imperative Configuration as Code (CaC) tool that automates system configuration, software installation, and application deployment through task-based YAML playbooks describing the steps to achieve a desired setup. Here, we use it to install Kubernetes and the Argo tools on our infrastructure after it is provisioned\n",
    "-   Argo CD: A declarative GitOps continuous delivery tool for Kubernetes that automatically syncs and deploys applications based on the desired state stored in Git repositories.\n",
    "-   Argo Workflows: A Kubernetes-native workflow engine where you define workflows, which execute tasks inside containers to run pipelines, jobs, or automation processes.\n",
    "\n",
    "**Note**: that we use Argo CD and Argo Workflows, which are tightly integrated with Kubernetes, because we are working in the context of a Kubernetes deployment. If our service was not deployed in Kubernetes (for example: it was deployed using “plain” Docker containers without a container orchestration framework), we would use other tools for managing the application and model lifecycle.\n",
    "\n",
    "To run this experiment, you should have already created an account on Chameleon, and become part of a project. You should also have added your SSH key to the KVM@TACC site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment topology\n",
    "\n",
    "In this experiment, we will deploy a 3-node Kubernetes cluster on Chameleon instances. The Kubernetes cluster will be self-managed, which means that the infrastructure provider is not responsbile for setting up and maintaining our cluster; *we* are.\n",
    "\n",
    "However, the cloud infrastructure provider will provide the compute resources and network resources that we need. We will provision the following resources for this experiment:\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/lab-topology.svg\" alt=\"Experiment topology.\" />\n",
    "<figcaption aria-hidden=\"true\">Experiment topology.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chi import server, context, lease\n",
    "import os\n",
    "\n",
    "context.version = \"1.0\" \n",
    "context.choose_project()\n",
    "context.choose_site(default=\"CHI@UC\")\n",
    "\n",
    "l = lease.get_lease(f\"compute_gigaio_project46\") # or llm_single_netID, or llm_multi_netID\n",
    "l.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = {\n",
    "#     \"node1\": {\n",
    "#         \"sharednet2\": \"sharednet2-port-id-node1\",\n",
    "#         \"private\": \"private-port-id-node1\"\n",
    "#     },\n",
    "#     \"node2\": {\n",
    "#         \"sharednet2\": \"sharednet2-port-id-node2\",\n",
    "#         \"private\": \"private-port-id-node2\"\n",
    "#     },\n",
    "#     \"node3\": {\n",
    "#         \"sharednet2\": \"sharednet2-port-id-node3\",\n",
    "#         \"private\": \"private-port-id-node3\"\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# for name, ports in nodes.items():\n",
    "#     server = conn.create_server(\n",
    "#         name=f\"{name}-mlops\",\n",
    "#         image='CC-Ubuntu24.04-CUDA',\n",
    "#         flavor='baremetal',\n",
    "#         key_name='your_key_name',\n",
    "#         nics=[\n",
    "#             {\"port-id\": ports[\"sharednet2\"]},\n",
    "#             {\"port-id\": ports[\"private\"]}\n",
    "#         ],\n",
    "#         wait=True,\n",
    "#         auto_ip=False\n",
    "#     )\n",
    "#     print(f\"{name} created with ID: {server.id}\")\n",
    "\n",
    "from openstack import connection\n",
    "\n",
    "conn = connection.from_config(cloud='openstack')\n",
    "\n",
    "server = conn.create_server(\n",
    "    name=\"node1-mlops\",\n",
    "    image='CC-Ubuntu24.04-CUDA',\n",
    "    flavor='baremetal',\n",
    "    key_name='id_rsa_chameleon',\n",
    "    nics=[{\"port-id\": \"60024d29-fc52-4a76-94f4-fcb032e79258\"}],\n",
    "    scheduler_hints={\"reservation\": \"c4b1a19e-3e15-4aa1-a74e-9d996a78c14a\"},\n",
    "    wait=False,\n",
    "    auto_ip=False\n",
    ")\n",
    "\n",
    "print(f\"Server created: {server.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = conn.get_server(\"5f818852-a9e0-4908-9cc0-bed1fb408567\")\n",
    "print(server.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision a key\n",
    "\n",
    "Before you begin, open this experiment on Trovi:\n",
    "\n",
    "-   Use this link: [MLOps Pipeline](https://chameleoncloud.org/experiment/share/1eb302de-4707-4ae9-ae2d-391b9b8e5261) on Trovi\n",
    "-   Then, click “Launch on Chameleon”. This will start a new Jupyter server for you, with the experiment materials already in it.\n",
    "\n",
    "You will see several notebooks inside the `mlops-chi` directory - look for the one titled `0_intro.ipynb`. Open this notebook and execute the following cell (and make sure the correct project is selected):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you may continue following along at [Build an MLOps Pipeline](https://teaching-on-testbeds.github.io/mlops-chi/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
